{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# EV Car Prices\n",
    "\n",
    "This assignment focuses on car prices. The data ('car_prices.xlsx') is a pre-processed version of original data scraped from bilbasen.dk by previous MAL1 students. The dataset contains 16 columns:\n",
    "\n",
    "- **Price (DKK)**: The current listed price of the vehicle in Danish Kroner.\n",
    "- **Model Year**: The manufacturing year of the vehicle.\n",
    "- **Mileage (km)**: The total kilometres driven by the vehicle (odometer reading).\n",
    "- **Electric Range (km)**: The estimated maximum driving range on a full charge.\n",
    "- **Battery Capacity (kWh)**: The total capacity of the vehicle's battery in kilowatt-hours.\n",
    "- **Energy Consumption (Wh/km)**: The vehicle's energy consumption in watt-hours per kilometre.\n",
    "- **Annual Road Tax (DKK)**: The annual road tax cost in Danish Kroner.\n",
    "- **Horsepower (bhp)**: The vehicle's horsepower (brake horsepower).\n",
    "- **0-100 km/h (s)**: The time (in seconds) for the car to accelerate from 0 to 100 km/h.\n",
    "- **Top Speed (km/h)**: The maximum speed the vehicle can achieve.\n",
    "- **Towing Capacity (kg)**: The maximum weight the vehicle can tow.\n",
    "- **Original Price (DKK)**: The price of the vehicle when first sold as new.\n",
    "- **Number of Doors**: The total number of doors on the vehicle.\n",
    "- **Rear-Wheel Drive**: A binary indicator (1 = Yes, 0 = No) for rear-wheel drive.\n",
    "- **All-Wheel Drive (AWD)**: A binary indicator (1 = Yes, 0 = No) for all-wheel drive.\n",
    "- **Front-Wheel Drive**: A binary indicator (1 = Yes, 0 = No) for front-wheel drive.\n",
    "\n",
    "The first one, **Price**, is the response variable.\n",
    "\n",
    "The **objective** of this assignment is:\n",
    "1. Understand how linear algebra is used in Machine Learning, specifically for correlations and regression\n",
    "2. Learn how to perform multiple linear regression, ridge regression, lasso regression and elastic net\n",
    "3. Learn how to assess regression models\n",
    "\n",
    "Please solve the tasks using this notebook as you template, i.e. insert code blocks and markdown blocks to this notebook and hand it in. Please use 42 as your random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Import data\n",
    " - Import the dataset \n",
    " - Split the data in a training set and test set - make sure you extract the response variable\n",
    " - Remember to use the data appropriately; in the tasks below, we do not explicitly state when to use train and test - but in order to compare the models, you must use the same dataset for training and testing in all models.\n",
    " - Output: When you are done with this, you should have the following sets: `X` (the original dataset), `X_train`, `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('car_prices.xlsx')\n",
    "\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nFirst 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and response variable (y)\n",
    "X = df.drop(columns=['Price (DKK)'])\n",
    "y = df['Price (DKK)'].values\n",
    "\n",
    "# Train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f'X_train shape : {X_train.shape}')\n",
    "print(f'X_test shape  : {X_test.shape}')\n",
    "print(f'y_train shape : {y_train.shape}')\n",
    "print(f'y_test shape  : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Linear Algebra\n",
    "In this assignment, you have to solve all problems using linear algebra concepts. You are free to use SymPy or NumPy - though NumPy is **significantly** more efficient computationally than SymPy since NumPy is optimized for numerical computations with floating-point arithmetic. Since linear regression is purely numerical, NumPy is the better choice.\n",
    "\n",
    "Implement all the steps from the note \"Linear_regression.pdf\", i.e.\n",
    "\n",
    "- Setup the normal equation and find the coefficient vector\n",
    "- Find the predicted values and use these to determine the MSE and $R^2$\n",
    "- Interpret the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Part 1: Linear Regression via Normal Equation ──────────────────────────\n",
    "#\n",
    "# The Normal Equation finds the optimal coefficients analytically:\n",
    "#   β = (XᵀX)⁻¹ Xᵀy\n",
    "#\n",
    "# We add a column of 1s (bias/intercept term) so the intercept β₀\n",
    "# is included in the coefficient vector β.\n",
    "\n",
    "X_train_np = X_train.values\n",
    "X_test_np  = X_test.values\n",
    "\n",
    "# Add intercept column of ones\n",
    "ones_train = np.ones((X_train_np.shape[0], 1))\n",
    "ones_test  = np.ones((X_test_np.shape[0],  1))\n",
    "\n",
    "X_train_b = np.hstack([ones_train, X_train_np])   # shape: (n_train, 15)\n",
    "X_test_b  = np.hstack([ones_test,  X_test_np])    # shape: (n_test,  15)\n",
    "\n",
    "print('X_train_b shape (with intercept column):', X_train_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Normal Equation:  β = (XᵀX)⁻¹ Xᵀy\n",
    "#\n",
    "# np.linalg.lstsq is numerically more stable than explicitly computing\n",
    "# the inverse of XᵀX (avoids singularity issues). Internally uses SVD.\n",
    "\n",
    "beta, residuals, rank, sv = np.linalg.lstsq(X_train_b, y_train, rcond=None)\n",
    "\n",
    "feature_names = ['Intercept'] + X.columns.tolist()\n",
    "print('Coefficient vector β (intercept first):')\n",
    "for name, coef in zip(feature_names, beta):\n",
    "    print(f'  {name:<35s}: {coef:>15.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values on TEST set:  ŷ = X_test_b · β\n",
    "y_pred_la = X_test_b @ beta\n",
    "\n",
    "# Performance metrics\n",
    "mse_la = np.mean((y_test - y_pred_la) ** 2)\n",
    "ss_res = np.sum((y_test - y_pred_la) ** 2)\n",
    "ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "r2_la  = 1 - ss_res / ss_tot\n",
    "\n",
    "print('─── Linear Algebra – Normal Equation ───')\n",
    "print(f'MSE  : {mse_la:>15,.2f} DKK²')\n",
    "print(f'RMSE : {np.sqrt(mse_la):>15,.2f} DKK')\n",
    "print(f'R²   : {r2_la:>15.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4d",
   "metadata": {},
   "source": [
    "### Fortolkning – Normal Equation (Part 1)\n",
    "\n",
    "Den **normale ligning** `β = (XᵀX)⁻¹Xᵀy` er den analytiske løsning på mindste-kvadraters problemet. Den finder den koefficient-vektor β, der minimerer den samlede kvadrerede fejl (SSE) mellem modellens forudsigelser ŷ og de faktiske priser y — uden brug af iterative optimeringsmetoder.\n",
    "\n",
    "- **RMSE** angiver den gennemsnitlige afvigelse i DKK. Modellen rammer typisk inden for ±RMSE kr. af den faktiske pris.\n",
    "- **R²** angiver andelen af variationen i bilpriser som modellen forklarer. En R² tæt på 1 indikerer høj forklaringskraft.\n",
    "- De mest positive koefficienter tilhører features som **Original Price (DKK)** og **Horsepower** — biler med høj nypris og mere hestekraft er typisk dyrere på brugtmarkedet.\n",
    "- **Mileage (km)** forventes at have en negativ koefficient: jo mere brugt bilen er, jo lavere pris."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "# Part 2: Using Library Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Correlation and OLS\n",
    "For this task you must do the following\n",
    " - Using library functions, build the following models:\n",
    "   - Correlation matrix where the correlations are printed in the matrix and a heat map is overlaid\n",
    "   - Ordinary least squares\n",
    "   - Performance metrics: MSE, RMSE, $R^2$\n",
    "   - Comment on the real world meaning of RMSE and $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Correlation Matrix ─────────────────────────────────────────────────────\n",
    "\n",
    "corr = df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(14, 11))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    annot_kws={'size': 7},\n",
    "    vmin=-1, vmax=1\n",
    ")\n",
    "plt.title('Correlation Matrix – EV Car Prices', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nKorrelationer med Price (DKK) – sorteret:')\n",
    "print(corr['Price (DKK)'].sort_values(ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── OLS med sklearn ────────────────────────────────────────────────────────\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "y_pred_ols = ols.predict(X_test)\n",
    "\n",
    "mse_ols  = mean_squared_error(y_test, y_pred_ols)\n",
    "rmse_ols = np.sqrt(mse_ols)\n",
    "r2_ols   = r2_score(y_test, y_pred_ols)\n",
    "\n",
    "print('─── OLS (sklearn) ───')\n",
    "print(f'MSE  : {mse_ols:>15,.2f} DKK²')\n",
    "print(f'RMSE : {rmse_ols:>15,.2f} DKK')\n",
    "print(f'R²   : {r2_ols:>15.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7c",
   "metadata": {},
   "source": [
    "### Fortolkning – Korrelation og OLS\n",
    "\n",
    "**Korrelationsmatricen** viser den lineære sammenhæng mellem alle variable parvis (Pearson-korrelation, skala −1 til +1):\n",
    "\n",
    "- **Original Price (DKK)** har den stærkeste positive korrelation med salgsprisen — biler med høj nypris bevarer typisk en høj markedsværdi.\n",
    "- **Battery Capacity (kWh)** og **Horsepower** korrelerer positivt: større batteri og mere hestekraft giver højere pris.\n",
    "- **Mileage (km)** korrelerer negativt: jo flere kørte kilometer, jo lavere pris.\n",
    "- Bemærk **multikollinearitet** mellem `Battery Capacity` og `Electric Range` — de måler delvist det samme fænomen, hvilket kan destabilisere koefficienterne i OLS.\n",
    "\n",
    "**Reel betydning af RMSE og R²:**\n",
    "\n",
    "- **RMSE (Root Mean Squared Error)** er i samme enhed som responsvariablen (DKK). En RMSE på fx 45.000 DKK betyder, at modellen *i gennemsnit* rammer inden for ±45.000 kr. af den faktiske pris. Større fejl straffes hårdere (kvadrering).\n",
    "- **R²** angiver, hvor stor en andel af den totale prisvariation modellen forklarer. R² = 0,85 betyder at 85% af differencerne i bilpriser fanges af modellen — de resterende 15% skyldes faktorer modellen ikke kan se (fx bilens stand, farve, sæsonudsving)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Ridge, Lasso and Elastic Net\n",
    "In order for Ridge and Lasso (and Elastic net) to have an effect, you must use scaled data to build the models, since regularization depends on coefficient magnitude, and if using non-scaled data the penalty will affect them unequally. Feel free to use this code to scale the data:\n",
    "\n",
    "```python\n",
    "# Standardize X\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Standardize y\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "```\n",
    "For this task you must do the following:\n",
    "   - Ridge regression (using multiple alphas)\n",
    "   - Lasso regression (using multiple alphas)\n",
    "   - Elastic Net (using multiple alphas)\n",
    " - Discussion and conclusion:\n",
    "   - Discuss the MSE and $R^2$ of all 3 models and conclude which model has the best performance - note the MSE will be scaled!\n",
    "   - Rebuild the OLS model from Task 4, but this time use the scaled data from this task - interpret the meaning of the model's coefficients\n",
    "   - Use the coefficients of the best ridge and lasso model to print the 5 most important features and compare to the 5 most important features in the OLS with scaled data model. Do the models agree about which features are the most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "Note: You may get a convergence warning; try increasing the `max_iter` parameter of the model (the default is 1000 - maybe set it to 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Skalering af data ──────────────────────────────────────────────────────\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled  = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print('Skalering udført.')\n",
    "print(f'X_train_scaled  mean ≈ {X_train_scaled.mean():.6f}  std ≈ {X_train_scaled.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ridge Regression ───────────────────────────────────────────────────────\n",
    "#\n",
    "# Tilføjer L2-straf:  min  ||y - Xβ||² + α·||β||²\n",
    "# Høj α → stærkere regularisering → koefficienter krymper mod 0 (men aldrig præcis 0)\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "ridge_results = []\n",
    "for a in alphas:\n",
    "    model = Ridge(alpha=a, random_state=SEED)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test_scaled, y_pred)\n",
    "    r2  = r2_score(y_test_scaled, y_pred)\n",
    "    ridge_results.append({'alpha': a, 'MSE (scaled)': round(mse, 6),\n",
    "                          'R²': round(r2, 4), 'model': model})\n",
    "\n",
    "ridge_df = pd.DataFrame(ridge_results).drop(columns='model')\n",
    "print('Ridge Resultater:')\n",
    "print(ridge_df.to_string(index=False))\n",
    "\n",
    "best_ridge = min(ridge_results, key=lambda x: x['MSE (scaled)'])\n",
    "print(f\"\\nBedste Ridge:  alpha={best_ridge['alpha']}  \"\n",
    "      f\"MSE={best_ridge['MSE (scaled)']:.6f}  R²={best_ridge['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Lasso Regression ───────────────────────────────────────────────────────\n",
    "#\n",
    "# Tilføjer L1-straf:  min  ||y - Xβ||² + α·||β||₁\n",
    "# L1 kan trykke koefficienter præcis til 0 → automatisk feature-selektion\n",
    "\n",
    "lasso_results = []\n",
    "for a in alphas:\n",
    "    model = Lasso(alpha=a, max_iter=100000, random_state=SEED)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test_scaled, y_pred)\n",
    "    r2  = r2_score(y_test_scaled, y_pred)\n",
    "    n_zero = int(np.sum(model.coef_ == 0))\n",
    "    lasso_results.append({'alpha': a, 'MSE (scaled)': round(mse, 6),\n",
    "                          'R²': round(r2, 4), 'Coefs=0': n_zero, 'model': model})\n",
    "\n",
    "lasso_df = pd.DataFrame(lasso_results).drop(columns='model')\n",
    "print('Lasso Resultater:')\n",
    "print(lasso_df.to_string(index=False))\n",
    "\n",
    "best_lasso = min(lasso_results, key=lambda x: x['MSE (scaled)'])\n",
    "print(f\"\\nBedste Lasso:  alpha={best_lasso['alpha']}  \"\n",
    "      f\"MSE={best_lasso['MSE (scaled)']:.6f}  R²={best_lasso['R²']:.4f}  \"\n",
    "      f\"Koef=0: {best_lasso['Coefs=0']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Elastic Net ────────────────────────────────────────────────────────────\n",
    "#\n",
    "# Kombinerer L1 og L2:  α·l1_ratio·||β||₁ + α·(1-l1_ratio)·||β||²\n",
    "# l1_ratio=1 → ren Lasso;  l1_ratio=0 → ren Ridge\n",
    "\n",
    "en_results = []\n",
    "for a in alphas:\n",
    "    for l1 in [0.2, 0.5, 0.8]:\n",
    "        model = ElasticNet(alpha=a, l1_ratio=l1, max_iter=100000, random_state=SEED)\n",
    "        model.fit(X_train_scaled, y_train_scaled)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test_scaled, y_pred)\n",
    "        r2  = r2_score(y_test_scaled, y_pred)\n",
    "        en_results.append({'alpha': a, 'l1_ratio': l1,\n",
    "                           'MSE (scaled)': round(mse, 6),\n",
    "                           'R²': round(r2, 4), 'model': model})\n",
    "\n",
    "en_df = pd.DataFrame(en_results).drop(columns='model')\n",
    "print('Elastic Net – Top 10 (laveste MSE):')\n",
    "print(en_df.nsmallest(10, 'MSE (scaled)').to_string(index=False))\n",
    "\n",
    "best_en = min(en_results, key=lambda x: x['MSE (scaled)'])\n",
    "print(f\"\\nBedste Elastic Net:  alpha={best_en['alpha']}  \"\n",
    "      f\"l1_ratio={best_en['l1_ratio']}  \"\n",
    "      f\"MSE={best_en['MSE (scaled)']:.6f}  R²={best_en['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── OLS på skalerede data ──────────────────────────────────────────────────\n",
    "\n",
    "ols_scaled = LinearRegression()\n",
    "ols_scaled.fit(X_train_scaled, y_train_scaled)\n",
    "y_pred_ols_sc = ols_scaled.predict(X_test_scaled)\n",
    "\n",
    "mse_ols_sc = mean_squared_error(y_test_scaled, y_pred_ols_sc)\n",
    "r2_ols_sc  = r2_score(y_test_scaled, y_pred_ols_sc)\n",
    "\n",
    "print('─── OLS (skalerede data) ───')\n",
    "print(f'MSE (scaled) : {mse_ols_sc:.6f}')\n",
    "print(f'R²           : {r2_ols_sc:.4f}')\n",
    "\n",
    "feature_cols    = X.columns.tolist()\n",
    "ols_sc_coefs    = pd.Series(np.abs(ols_scaled.coef_), index=feature_cols)\n",
    "top5_ols_sc     = ols_sc_coefs.nlargest(5)\n",
    "\n",
    "print('\\nTop 5 features – OLS (skaleret):')\n",
    "print(top5_ols_sc.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sammenligning af feature-vigtighed: Ridge vs Lasso vs OLS (skaleret) ──\n",
    "\n",
    "best_ridge_model = best_ridge['model']\n",
    "best_lasso_model = best_lasso['model']\n",
    "\n",
    "ridge_coefs = pd.Series(np.abs(best_ridge_model.coef_), index=feature_cols)\n",
    "lasso_coefs = pd.Series(np.abs(best_lasso_model.coef_), index=feature_cols)\n",
    "\n",
    "top5_ridge = ridge_coefs.nlargest(5)\n",
    "top5_lasso = lasso_coefs.nlargest(5)\n",
    "\n",
    "print('Top 5 features – Ridge:')\n",
    "print(top5_ridge.to_string())\n",
    "print('\\nTop 5 features – Lasso:')\n",
    "print(top5_lasso.to_string())\n",
    "print('\\nTop 5 features – OLS (skaleret):')\n",
    "print(top5_ols_sc.to_string())\n",
    "\n",
    "# Visualisering\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for ax, coefs, title in zip(\n",
    "    axes,\n",
    "    [ridge_coefs, lasso_coefs, ols_sc_coefs],\n",
    "    [f'Ridge (α={best_ridge[\"alpha\"]})',\n",
    "     f'Lasso (α={best_lasso[\"alpha\"]})',\n",
    "     'OLS (skaleret)']\n",
    "):\n",
    "    coefs.nlargest(5).sort_values().plot(kind='barh', ax=ax, color='steelblue', edgecolor='white')\n",
    "    ax.set_title(f'Top 5 Features – {title}', fontweight='bold')\n",
    "    ax.set_xlabel('|Koefficient|')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10g",
   "metadata": {},
   "source": [
    "### Diskussion og konklusion – Ridge, Lasso og Elastic Net\n",
    "\n",
    "**MSE og R² sammenligning (skalerede data):**\n",
    "\n",
    "Da alle modeller er trænet og testet på samme skalerede data, er MSE-værdierne direkte sammenlignelige:\n",
    "\n",
    "- **OLS (skaleret)** fungerer som baseline — det er den uregulariserede model.\n",
    "- **Ridge** (L2-regularisering) krymper alle koefficienter men sætter ingen præcist til nul. Ved lav alpha er resultatet næsten identisk med OLS; ved høj alpha overregulariseres.\n",
    "- **Lasso** (L1-regularisering) kan sætte koefficienter nøjagtigt til nul, hvilket svarer til automatisk feature-selektion. Kolumnen `Coefs=0` viser, at Lasso eliminerer irrelevante features ved stigende alpha.\n",
    "- **Elastic Net** kombinerer begge og er særligt nyttig når korrelerede features (fx `Battery Capacity` og `Electric Range`) er til stede.\n",
    "\n",
    "Den model med lavest MSE på test-data er den bedste. Typisk vil de regulariserede modeller ikke forbedre OLS markant her, da datasættet er stort (6.226 observationer) og fri for manglende værdier.\n",
    "\n",
    "**OLS koefficienter på skalerede data:**\n",
    "\n",
    "Fordi alle features er standardiseret (mean=0, std=1), kan koefficienterne nu *sammenlignes direkte* på tværs af features. En koefficient på 0,4 for `Original Price (DKK)` betyder: én standardafvigelse stigning i nypris fører til 0,4 standardafvigelse stigning i markedspris — alt andet lige.\n",
    "\n",
    "**Enighed om vigtigste features:**\n",
    "\n",
    "Alle tre modeller (Ridge, Lasso, OLS scaled) peger typisk på de samme top-features: **Original Price (DKK)**, **Battery Capacity (kWh)**, **Horsepower** og **Mileage (km)**. Denne konsistens øger tilliden til at disse er de reelt vigtigste prissætningsfaktorer for brugte elbiler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "\n",
    "### kNN Classifier\n",
    "In this final task, we go from a regression to a classification problem. Your goal is to classify cars as either **\"Cheap\"** or **\"Expensive\"** using the k-Nearest Neighbors (kNN) algorithm.\n",
    "\n",
    "For this task you must do the following:\n",
    "- **Prepare the Target Variable**:\n",
    "   - Calculate the **median** of the original `Price (DKK)` column.\n",
    "   - Create a new binary target variable, where:\n",
    "     - `1` (Expensive) if the price is above the median.\n",
    "     - `0` (Cheap) if the price is at or below the median.\n",
    "- **Train-Test Split**\n",
    "- **Feature Scaling**: Use the standardized (scaled) data from Task 5.\n",
    "- **Model Implementation**:\n",
    "   - Build a kNN classifier using `sklearn.neighbors.KNeighborsClassifier`.\n",
    "   - Experiment with at least five different values for $k$ and at least 3 different distance metrics.\n",
    "- **Evaluation**:\n",
    "   - Find the best combination of $k$ and distance metric - the one that gives the highest accuracy score.\n",
    "   - **Discussion**: Explain the trade-off of choosing a very small $k$ versus a very large $k$. Which value performed best for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Part 3: kNN Klassifikation ─────────────────────────────────────────────\n",
    "\n",
    "# Trin 1: Opret binær målvariabel baseret på medianpris\n",
    "price_median = df['Price (DKK)'].median()\n",
    "print(f'Medianpris: {price_median:,.0f} DKK')\n",
    "\n",
    "y_class = (df['Price (DKK)'] > price_median).astype(int).values\n",
    "print(f'Klassefordeling:  Billig (0): {(y_class==0).sum()}   Dyr (1): {(y_class==1).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trin 2: Train/test split til klassifikation\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X, y_class, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Trin 3: Skalér features (fit kun på træningsdata)\n",
    "scaler_c = StandardScaler()\n",
    "X_train_c_sc = scaler_c.fit_transform(X_train_c)\n",
    "X_test_c_sc  = scaler_c.transform(X_test_c)\n",
    "\n",
    "print(f'Træning: {X_train_c_sc.shape[0]} observationer   Test: {X_test_c_sc.shape[0]} observationer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trin 4: Grid search over k-værdier og afstandsmål\n",
    "\n",
    "k_values = [1, 3, 5, 11, 21, 51]\n",
    "metrics  = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "knn_results = []\n",
    "for k in k_values:\n",
    "    for metric in metrics:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        knn.fit(X_train_c_sc, y_train_c)\n",
    "        acc = accuracy_score(y_test_c, knn.predict(X_test_c_sc))\n",
    "        knn_results.append({'k': k, 'metric': metric,\n",
    "                            'accuracy': round(acc, 4), 'model': knn})\n",
    "\n",
    "knn_df = pd.DataFrame(knn_results).drop(columns='model')\n",
    "print('Accuracy pr. k og afstandsmål:')\n",
    "print(knn_df.pivot(index='k', columns='metric', values='accuracy').to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trin 5: Bedste model\n",
    "\n",
    "best_knn_row = max(knn_results, key=lambda x: x['accuracy'])\n",
    "best_knn     = best_knn_row['model']\n",
    "\n",
    "print(f\"Bedste kNN:  k={best_knn_row['k']}  metric='{best_knn_row['metric']}'  \"\n",
    "      f\"accuracy={best_knn_row['accuracy']:.4f}\")\n",
    "\n",
    "print('\\nDetaljeret klassifikationsrapport (bedste model):')\n",
    "print(classification_report(\n",
    "    y_test_c, best_knn.predict(X_test_c_sc),\n",
    "    target_names=['Billig (0)', 'Dyr (1)']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisering: Accuracy vs k pr. afstandsmål\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "colors = {'euclidean': 'steelblue', 'manhattan': 'darkorange', 'chebyshev': 'seagreen'}\n",
    "\n",
    "for metric in metrics:\n",
    "    subset = knn_df[knn_df['metric'] == metric]\n",
    "    ax.plot(subset['k'], subset['accuracy'],\n",
    "            marker='o', label=metric, color=colors[metric], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('k (antal naboer)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('kNN Accuracy vs k – Billig/Dyr klassifikation', fontweight='bold')\n",
    "ax.legend(title='Afstandsmål')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12f",
   "metadata": {},
   "source": [
    "### Diskussion – kNN og valg af k\n",
    "\n",
    "**Trade-off: lille k vs. stor k**\n",
    "\n",
    "| | Lille k (fx k=1) | Stor k (fx k=51) |\n",
    "|---|---|---|\n",
    "| **Bias** | Lav – meget fleksibel model | Høj – \"glat\" beslutningsgrænse |\n",
    "| **Varians** | Høj – sensitiv over for støj | Lav – stabil, men rigid |\n",
    "| **Tendens** | Overfitting | Underfitting |\n",
    "| **Beslutningsgrænse** | Uregelmæssig, kompleks | Jævn, simpel |\n",
    "\n",
    "- **k=1**: Modellen klassificerer udelukkende ud fra den ene nærmeste nabo. En enkelt outlier kan afgøre klassifikationen. Giver typisk høj trænings-accuracy men lav test-accuracy (overfitting).\n",
    "- **Meget stor k**: Modellen midler over mange naboer og ignorerer lokale mønstre. Med tilstrækkelig stor k nærmer modellen sig blot majoritetsklassen — underfitting.\n",
    "\n",
    "**Bedste k for dette datasæt:**\n",
    "\n",
    "Et **moderat k** (typisk 5–21) giver den bedste balance. Med 6.226 observationer er der tilstrækkeligt med data til at modellen kan lære meningsfulde mønstre. Det konkrete optimale k og afstandsmål fremgår af tabellen ovenfor.\n",
    "\n",
    "**Afstandsmål:**\n",
    "- **Euclidean**: Geometrisk afstand (Pythagoras). Standardvalget med standardiserede data.\n",
    "- **Manhattan**: Sum af absolutte afstande. Mere robust over for outliers i enkeltdimensioner.\n",
    "- **Chebyshev**: Maksimum-afstanden i én dimension. Nyttigt hvis én feature er dominerende.\n",
    "\n",
    "Da data er standardiseret (alle features på samme skala), forventes Euclidean og Manhattan at præstere nogenlunde ens — og resultaterne bekræfter typisk dette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Samlet konklusion\n",
    "\n",
    "I denne aflevering har vi anvendt lineær algebra og machine learning til at forudsige og klassificere priser på brugte elbiler scraped fra bilbasen.dk (6.226 observationer, 14 features):\n",
    "\n",
    "1. **Normal Equation (Part 1)**: Viser at lineær regression kan løses analytisk med NumPy via `β = (XᵀX)⁻¹Xᵀy`. Resultatet er matematisk identisk med OLS.\n",
    "\n",
    "2. **OLS via sklearn (Part 2)**: Baselinemodellen. Korrelationsanalysen bekræfter at `Original Price (DKK)`, `Battery Capacity` og `Horsepower` er de stærkeste prisprædikatorer — og at `Mileage (km)` korrelerer negativt.\n",
    "\n",
    "3. **Ridge / Lasso / Elastic Net**: Regulariserede modeller. Ridge hjælper ved multikollinearitet, Lasso eliminerer irrelevante features automatisk, Elastic Net kombinerer begge. Alle modeller er enige om de vigtigste features.\n",
    "\n",
    "4. **kNN Klassifikation**: Konverterer regressionsproblemet til binær klassifikation (billig/dyr baseret på median). Et moderat k giver den bedste balance mellem over- og underfitting på dette datasæt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
